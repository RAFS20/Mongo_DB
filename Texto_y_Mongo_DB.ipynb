{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNmR+1db92cXKltkAGhtCr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAFS20/Mongo_DB/blob/main/Texto_y_Mongo_DB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuaderno de: Ricardo Alonzo Fernández Salguero"
      ],
      "metadata": {
        "id": "lVz3LzcaWSba"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2Jn2b0DOawO",
        "outputId": "97756c0a-b4b7-48a5-f7ad-a36e8a0fac73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.3/671.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.4.2 pymongo-4.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import quote_plus\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "\n",
        "# Tu nombre de usuario y contraseña en bruto\n",
        "username = \"clasesmongo\"\n",
        "password = \"!!4egt9a!pUb#@E\"\n",
        "\n",
        "# Codifica porcentaje el nombre de usuario y la contraseña\n",
        "encoded_username = quote_plus(username)\n",
        "encoded_password = quote_plus(password)\n",
        "\n",
        "# Construye la URI de MongoDB\n",
        "uri = f\"mongodb+srv://{encoded_username}:{encoded_password}@cluster0.segsxqx.mongodb.net/?retryWrites=true&w=majority\"\n",
        "\n",
        "# Crea un nuevo cliente y conecta al servidor\n",
        "try:\n",
        "    client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "    # Envía un ping para confirmar una conexión exitosa\n",
        "    client.admin.command('ping')\n",
        "    print(\"Hiciste ping a tu implementación. ¡Te has conectado con éxito a MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAeiGO2bSGVw",
        "outputId": "17aab86c-517a-49a0-c4ca-0d5b106418d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hiciste ping a tu implementación. ¡Te has conectado con éxito a MongoDB!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk pymongo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqdVepmdU2O-",
        "outputId": "03a99f81-b02a-4dc0-f830-09bf687e8325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo) (2.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de texto\n",
        "\n",
        "Este script Python se encarga de conectarse a una base de datos MongoDB, procesar un texto de ejemplo, realizar un análisis de sentimiento, crear un mapeo semántico de palabras en el texto y almacenar estos datos en la base de datos. También muestra los resultados del análisis de sentimiento y las entidades nombradas en el texto.\n",
        "\n",
        "**Importación de bibliotecas:**\n",
        "\n",
        "```python\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pymongo import MongoClient\n",
        "from urllib.parse import quote_plus\n",
        "from pymongo.server_api import ServerApi\n",
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "from textblob import download_corpora\n",
        "download_corpora.main()\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "```\n",
        "\n",
        "- `os`: Importa la biblioteca os, que proporciona funciones para interactuar con el sistema operativo, aunque no se utiliza en este código.\n",
        "\n",
        "- `nltk`: Importa la biblioteca Natural Language Toolkit (NLTK), una biblioteca de procesamiento de lenguaje natural.\n",
        "\n",
        "- `stopwords`: Importa la lista de palabras de detención (stopwords) de NLTK. Estas son palabras comunes que suelen eliminarse en el procesamiento de texto porque no aportan mucho significado.\n",
        "\n",
        "- `word_tokenize`: Importa la función `word_tokenize` de NLTK, que se utiliza para dividir el texto en palabras o tokens.\n",
        "\n",
        "- `pymongo`: Importa la biblioteca pymongo, que permite interactuar con bases de datos MongoDB desde Python.\n",
        "\n",
        "- `quote_plus`: Importa la función `quote_plus` de urllib.parse, que se utiliza para codificar las credenciales de acceso a MongoDB en una URI.\n",
        "\n",
        "- `ServerApi`: Importa la clase `ServerApi` de pymongo.server_api, que se utiliza para especificar la versión del servidor de MongoDB a utilizar.\n",
        "\n",
        "- `Counter`: Importa la clase `Counter` de la biblioteca collections, que se utiliza para contar la frecuencia de las palabras en el texto.\n",
        "\n",
        "- `TextBlob`: Importa la clase `TextBlob` de la biblioteca textblob, que se utiliza para realizar análisis de sentimiento en el texto.\n",
        "\n",
        "- `download_corpora.main()`: Llama a la función `download_corpora.main()` para descargar recursos adicionales que NLTK necesita, como listas de palabras y modelos lingüísticos.\n",
        "\n",
        "- `nltk.download('punkt')`: Descarga el tokenizer de NLTK llamado \"punkt\", que se utiliza para dividir el texto en oraciones y palabras.\n",
        "\n",
        "- `nltk.download('stopwords')`: Descarga la lista de palabras de detención en español que NLTK proporciona.\n",
        "\n",
        "**Definición de funciones:**\n",
        "\n",
        "```python\n",
        "def connect_to_mongo_db():\n",
        "    # Coloca tus credenciales aquí\n",
        "    username = \"clasesmongo\"\n",
        "    password = \"!!4egt9a!pUb#@E\"\n",
        "    encoded_username = quote_plus(username)\n",
        "    encoded_password = quote_plus(password)\n",
        "    uri = f\"mongodb+srv://{encoded_username}:{encoded_password}@cluster0.segsxqx.mongodb.net/?retryWrites=true&w=majority\"\n",
        "    client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "    client.admin.command('ping')\n",
        "    return client[\"semantic_mapping_db\"][\"text_data\"]\n",
        "```\n",
        "\n",
        "Esta función, `connect_to_mongo_db()`, se encarga de conectarse a una base de datos MongoDB. Aquí están los pasos detallados:\n",
        "\n",
        "- Define `username` y `password` con las credenciales para acceder a la base de datos MongoDB.\n",
        "\n",
        "- Utiliza `quote_plus` para codificar las credenciales de usuario y contraseña en la URI de conexión.\n",
        "\n",
        "- Construye la URI de conexión utilizando las credenciales codificadas y la dirección del servidor MongoDB.\n",
        "\n",
        "- Crea un cliente de MongoDB utilizando la URI y especifica que se utilizará la versión del servidor 1 mediante `ServerApi('1')`.\n",
        "\n",
        "- Realiza una prueba de conexión al servidor MongoDB con `client.admin.command('ping')` para asegurarse de que la conexión sea exitosa.\n",
        "\n",
        "- Devuelve una referencia a la colección \"text_data\" en la base de datos \"semantic_mapping_db\" del servidor MongoDB.\n",
        "\n",
        "```python\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = stopwords.words(\"spanish\")\n",
        "    return [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "```\n",
        "\n",
        "La función `preprocess_text(text)` toma un texto como entrada y realiza el preprocesamiento del texto. Aquí están los pasos detallados:\n",
        "\n",
        "- Tokeniza el texto en palabras utilizando `word_tokenize` y convierte todas las palabras a minúsculas.\n",
        "\n",
        "- Obtiene una lista de palabras de detención en español utilizando `stopwords.words(\"spanish\")`.\n",
        "\n",
        "- Devuelve una lista de palabras que son alfanuméricas y que no están en la lista de palabras de detención. Esto elimina palabras no informativas y caracteres especiales.\n",
        "\n",
        "```python\n",
        "def create_semantic_mapping(text):\n",
        "    tokens = preprocess_text(text)\n",
        "    frequency = Counter(tokens)\n",
        "    return dict(frequency)\n",
        "```\n",
        "\n",
        "La función `create_semantic_mapping(text)` toma un texto como entrada y crea un mapeo semántico de las palabras y su frecuencia en el texto. Aquí están los pasos detallados:\n",
        "\n",
        "- Utiliza la función `preprocess_text` para obtener una lista de tokens procesados a partir del texto.\n",
        "\n",
        "- Utiliza la clase `Counter` para contar la frecuencia de cada palabra en la lista de tokens.\n",
        "\n",
        "- Devuelve un diccionario que contiene las palabras como claves y sus frecuencias como valores.\n",
        "\n",
        "```python\n",
        "def analyze_text(text):\n",
        "    sentiment_analysis = TextBlob(text)\n",
        "    sentiment = sentiment_analysis.sentiment\n",
        "    print(\"Análisis de Sentimiento:\")\n",
        "    print(f\"Polaridad: {sentiment.polarity}\")\n",
        "    print(f\"Subjetividad: {sentiment.subjectivity}\")\n",
        "    entities = sentiment_analysis.noun_phrases\n",
        "    print(\"\\nEntidades Nombradas:\")\n",
        "    for entity in entities:\n",
        "        print(entity)\n",
        "```\n",
        "\n",
        "La función `analyze_text(text)` realiza un análisis de sentimiento y de entidades nombradas en el texto. Aquí están los pasos detallados:\n",
        "\n",
        "- Crea una instancia de `TextBlob` a partir del texto, que permite realizar análisis de sentimiento y de entidades nombradas.\n",
        "\n",
        "- Calcula el sentimiento del texto utilizando `sentiment_analysis.sentiment` y muestra la polaridad y la subjetividad del texto.\n",
        "\n",
        "- Imprime los resultados del análisis de sentimiento.\n",
        "\n",
        "- Obtiene las entidades nombradas del texto utilizando `sentiment_analysis.noun_phrases` y las imprime.\n",
        "\n",
        "**Función principal `main()`:**\n",
        "\n",
        "```python\n",
        "def main():\n",
        "    try:\n",
        "        collection = connect_to_mongo_db()\n",
        "        sample_text = \"El análisis de datos es una disciplina emocionante y valiosa en el campo de la inteligencia artificial.\"\n",
        "        semantic_mapping_result = create_semantic_mapping(sample_text)\n",
        "        document = {\n",
        "            \"text\": sample_text,\n",
        "            \"semantic_mapping\": semantic_mapping_result\n",
        "        }\n",
        "        inserted_doc = collection.insert_one(document)\n",
        "        retrieved_document = collection.find_one({\"_id\": inserted_doc.insert\n",
        "\n",
        "ed_id})\n",
        "        print(\"Texto Original:\")\n",
        "        print(retrieved_document[\"text\"])\n",
        "        print(\"\\nMapeo Semántico:\")\n",
        "        print(retrieved_document[\"semantic_mapping\"])\n",
        "        analyze_text(sample_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error: {e}\")\n",
        "```\n",
        "\n",
        "La función `main()` es la función principal del script. Aquí están los pasos detallados:\n",
        "\n",
        "- Dentro de un bloque `try`, se llama a `connect_to_mongo_db()` para obtener una referencia a la colección de MongoDB.\n",
        "\n",
        "- Se define un texto de ejemplo en la variable `sample_text`.\n",
        "\n",
        "- Se llama a `create_semantic_mapping(sample_text)` para crear un mapeo semántico de las palabras en el texto y se almacena en `semantic_mapping_result`.\n",
        "\n",
        "- Se crea un documento en forma de diccionario que contiene el texto original y su mapeo semántico.\n",
        "\n",
        "- Se inserta el documento en la colección de MongoDB utilizando `collection.insert_one(document)` y se almacena el resultado en `inserted_doc`.\n",
        "\n",
        "- Se recupera el documento recién insertado de la colección utilizando `collection.find_one({\"_id\": inserted_doc.inserted_id})` y se almacena en `retrieved_document`.\n",
        "\n",
        "- Se imprime el texto original y su mapeo semántico.\n",
        "\n",
        "- Finalmente, se llama a `analyze_text(sample_text)` para realizar un análisis de sentimiento y mostrar las entidades nombradas en el texto.\n",
        "\n",
        "- Si ocurre alguna excepción durante la ejecución, se maneja en el bloque `except` y se imprime un mensaje de error.\n",
        "\n",
        "**Ejecución del script:**\n",
        "\n",
        "```python\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```\n",
        "\n",
        "La última parte del código verifica si el script se está ejecutando como un programa independiente (`__name__ == \"__main__\"`) y, en caso afirmativo, llama a la función `main()` para ejecutar el código principal del programa.\n",
        "\n"
      ],
      "metadata": {
        "id": "6fSnbA_rbB-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pymongo import MongoClient\n",
        "from urllib.parse import quote_plus\n",
        "from pymongo.server_api import ServerApi\n",
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "from textblob import download_corpora\n",
        "\n",
        "download_corpora.main()\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def connect_to_mongo_db():\n",
        "    # Coloca tus credenciales aquí\n",
        "    username = \"clasesmongo\"\n",
        "    password = \"!!4egt9a!pUb#@E\"\n",
        "\n",
        "    encoded_username = quote_plus(username)\n",
        "    encoded_password = quote_plus(password)\n",
        "    uri = f\"mongodb+srv://{encoded_username}:{encoded_password}@cluster0.segsxqx.mongodb.net/?retryWrites=true&w=majority\"\n",
        "\n",
        "    client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "    client.admin.command('ping')\n",
        "    return client[\"semantic_mapping_db\"][\"text_data\"]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = stopwords.words(\"spanish\")\n",
        "    return [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "def create_semantic_mapping(text):\n",
        "    tokens = preprocess_text(text)\n",
        "    frequency = Counter(tokens)\n",
        "    return dict(frequency)\n",
        "\n",
        "def analyze_text(text):\n",
        "    sentiment_analysis = TextBlob(text)\n",
        "    sentiment = sentiment_analysis.sentiment\n",
        "    print(\"Análisis de Sentimiento:\")\n",
        "    print(f\"Polaridad: {sentiment.polarity}\")\n",
        "    print(f\"Subjetividad: {sentiment.subjectivity}\")\n",
        "\n",
        "    entities = sentiment_analysis.noun_phrases\n",
        "    print(\"\\nEntidades Nombradas:\")\n",
        "    for entity in entities:\n",
        "        print(entity)\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        collection = connect_to_mongo_db()\n",
        "        sample_text = \"El análisis de datos es una disciplina emocionante y valiosa en el campo de la inteligencia artificial, En la actualidad, la inteligencia artificial abarca una gran variedad de subcampos. Éstos van desde áreas de propósito general, aprendizaje y percepción, a otras más específicas como el reconocimiento de voz, el juego de ajedrez, la demostración de teoremas matemáticos, la escritura de poesía y el diagnóstico de enfermedades. La inteligencia artificial sintetiza y automatiza tareas que en principio son intelectuales y, por lo tanto, es potencialmente relevante para cualquier ámbito de diversas actividades intelectuales humanas.\"\n",
        "        semantic_mapping_result = create_semantic_mapping(sample_text)\n",
        "\n",
        "        document = {\n",
        "            \"text\": sample_text,\n",
        "            \"semantic_mapping\": semantic_mapping_result\n",
        "        }\n",
        "\n",
        "        inserted_doc = collection.insert_one(document)\n",
        "        retrieved_document = collection.find_one({\"_id\": inserted_doc.inserted_id})\n",
        "\n",
        "        print(\"Texto Original:\")\n",
        "        print(retrieved_document[\"text\"])\n",
        "        print(\"\\nMapeo Semántico:\")\n",
        "        print(retrieved_document[\"semantic_mapping\"])\n",
        "\n",
        "        analyze_text(sample_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlfPTb0JWklO",
        "outputId": "846da820-4f3d-4b02-8a4b-c9e293910219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished.\n",
            "Texto Original:\n",
            "El análisis de datos es una disciplina emocionante y valiosa en el campo de la inteligencia artificial, En la actualidad, la inteligencia artificial abarca una gran variedad de subcampos. Éstos van desde áreas de propósito general, aprendizaje y percepción, a otras más específicas como el reconocimiento de voz, el juego de ajedrez, la demostración de teoremas matemáticos, la escritura de poesía y el diagnóstico de enfermedades. La inteligencia artificial sintetiza y automatiza tareas que en principio son intelectuales y, por lo tanto, es potencialmente relevante para cualquier ámbito de diversas actividades intelectuales humanas.\n",
            "\n",
            "Mapeo Semántico:\n",
            "{'análisis': 1, 'datos': 1, 'disciplina': 1, 'emocionante': 1, 'valiosa': 1, 'campo': 1, 'inteligencia': 3, 'artificial': 3, 'actualidad': 1, 'abarca': 1, 'gran': 1, 'variedad': 1, 'subcampos': 1, 'éstos': 1, 'van': 1, 'áreas': 1, 'propósito': 1, 'general': 1, 'aprendizaje': 1, 'percepción': 1, 'específicas': 1, 'reconocimiento': 1, 'voz': 1, 'juego': 1, 'ajedrez': 1, 'demostración': 1, 'teoremas': 1, 'matemáticos': 1, 'escritura': 1, 'poesía': 1, 'diagnóstico': 1, 'enfermedades': 1, 'sintetiza': 1, 'automatiza': 1, 'tareas': 1, 'principio': 1, 'intelectuales': 2, 'potencialmente': 1, 'relevante': 1, 'cualquier': 1, 'ámbito': 1, 'diversas': 1, 'actividades': 1, 'humanas': 1}\n",
            "Análisis de Sentimiento:\n",
            "Polaridad: -0.4375\n",
            "Subjetividad: 0.875\n",
            "\n",
            "Entidades Nombradas:\n",
            "el\n",
            "datos es una disciplina emocionante y valiosa\n",
            "el campo\n",
            "en\n",
            "artificial abarca una gran variedad\n",
            "éstos van desde áreas\n",
            "propósito general\n",
            "aprendizaje y percepción\n",
            "otras más específicas como el reconocimiento\n",
            "el juego\n",
            "teoremas matemáticos\n",
            "poesía y el diagnóstico\n",
            "artificial sintetiza y automatiza tareas que\n",
            "principio son intelectuales y\n",
            "por lo tanto\n",
            "es potencialmente relevante para cualquier ámbito\n",
            "diversas actividades intelectuales humanas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Mapeo Semántico:\n",
        "   El mapeo semántico muestra la frecuencia de aparición de cada palabra en el texto original. En este caso, se muestra que las siguientes palabras tienen una frecuencia de aparición de 1:\n",
        "   - 'análisis'\n",
        "   - 'datos'\n",
        "   - 'disciplina'\n",
        "   - 'emocionante'\n",
        "   - 'valiosa'\n",
        "   - 'campo'\n",
        "   - 'inteligencia'\n",
        "   - 'artificial'\n",
        "\n",
        "   Esto proporciona información sobre las palabras clave presentes en el texto original.\n",
        "\n",
        "2. Análisis de Sentimiento:\n",
        "   El análisis de sentimiento evalúa la polaridad y subjetividad del texto. En este caso, se obtiene una polaridad de -0.6 y una subjetividad de 1.0.\n",
        "   - Polaridad: -0.6 indica que el texto tiene una orientación negativa. Esto sugiere que el texto contiene algún grado de negatividad o crítica.\n",
        "   - Subjetividad: 1.0 indica que el texto es altamente subjetivo, lo que significa que probablemente contiene opiniones personales o juicios en lugar de hechos objetivos.\n",
        "\n",
        "   En resumen, el análisis de sentimiento sugiere que el texto tiene un tono negativo y es altamente subjetivo.\n",
        "\n",
        "3. Entidades Nombradas:\n",
        "   Las entidades nombradas identificadas en el texto son \"el\" y \"el campo\". Sin embargo, estas identificaciones no parecen ser precisas o relevantes en el contexto del texto original. Es posible que haya un error en la identificación de entidades nombradas.\n",
        "\n",
        "En general, los resultados indican que el texto original contiene palabras clave relacionadas con la inteligencia artificial y la disciplina del análisis de datos. El análisis de sentimiento sugiere una orientación negativa y alta subjetividad en el texto, aunque esto podría no ser apropiado dado el contenido del texto original. La identificación de entidades nombradas no parece ser precisa en este caso."
      ],
      "metadata": {
        "id": "avI-BqbagYmv"
      }
    }
  ]
}